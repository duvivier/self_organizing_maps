{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Organizing Maps (SOMs) Notebook\n",
    "## Data extraction step - Step 1\n",
    "\n",
    "**Notebook by Maria J. Molina (NCAR) and Alice DuVivier (NCAR).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook reads in data from the CESM2-LE for a user-specified variable. It subsets the data by a user-specified coastal region around Antarctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed imports\n",
    "\n",
    "from minisom import MiniSom, asymptotic_decay\n",
    "import xarray as xr\n",
    "import cftime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "from datetime import timedelta\n",
    "from itertools import product\n",
    "\n",
    "import intake\n",
    "from distributed import Client\n",
    "from ncar_jobqueue import NCARCluster\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start up dask\n",
    "\n",
    "cluster = NCARCluster(memory='100 GB')\n",
    "cluster.scale(40) # number of workers requested\n",
    "#cluster.adapt(1,80) # min and max\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set user-specified information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set region of interest\n",
    "# needed for plotting and choosing a mask\n",
    "titles     = ['Ross Sea', 'Amundsen Bellingshausen Sea', 'Weddell Sea', 'Pacific Ocean', 'Indian Ocean']\n",
    "shorts     = ['Ross', 'AMB', 'Wed', 'Pac', 'Ind']\n",
    "masks      = ['Ross_mask', 'BAm_mask', 'Wed_mask', 'Pac_mask', 'Ind_mask']\n",
    "lat_maxes  = [-72, -65, -65, -60, -60] \n",
    "lat_mins   = [-85, -85, -85, -80, -80]\n",
    "lon_maxes  = [200, 300, 300, 90, 160] \n",
    "lon_mins   = [160, 220, 20, 20, 90]\n",
    "lon_avgs   = [190, 260, 340, 55, 125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set s, which is the paired values above \n",
    "s = 0\n",
    "sector_title = titles[s]\n",
    "sector_short = shorts[s]\n",
    "mask_in = masks[s]\n",
    "lat_max = lat_maxes[s]\n",
    "lat_min = lat_mins[s]\n",
    "lon_max = lon_maxes[s]\n",
    "lon_min = lon_mins[s]\n",
    "lon_avg = lon_avgs[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and get correct training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_file = '/glade/collections/cmip/catalog/intake-esm-datastore/catalogs/glade-cesm2-le.json'\n",
    "\n",
    "cat = intake.open_esm_datastore(catalog_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some info for the CESM2-LE data\n",
    "# set: variable to test, the location of the data, which ensemble member\n",
    "var_in = 'hi_d'\n",
    " # do not want smbb data\n",
    "forcing = 'cmip6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = cat.search(variable=var_in, forcing_variant=forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset\n",
    "subset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make arrays of half (25) of the CESM2-LE members \n",
    "# select every other from the large ensemble of both macro and micro starts\n",
    "# note that the naming of the files (YYYY.#### e.g. 1001.001) doesn't match the member_id directly, \n",
    "# but the ensemble number (### e.g. 001) does match the member_id field r? directly. So use this to search\n",
    "\n",
    "# set list of members from the dataset\n",
    "member_ids = subset.df.member_id.unique()\n",
    "\n",
    "# set list of members to KEEP\n",
    "keep_list = ['r1i', 'r3i', 'r5i','r7i', 'r9i']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_keep = [] # make a list to fill\n",
    "\n",
    "for member in keep_list:\n",
    "    for member_id in member_ids:\n",
    "        if member in member_id:\n",
    "            member_keep.append(member_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that we're keeping the right ones\n",
    "member_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reduce subset based on just the members to keep\n",
    "subset = subset.search(member_id=member_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#actually load the data we selected into a dataset\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "    dsets = subset.to_dataset_dict(cdf_kwargs={'chunks': {'time':240}, 'decode_times': True})\n",
    "\n",
    "#dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print names of the dataset keys, which refer to each of the ensembles loaded\n",
    "dsets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just one dataset key to see what it looks like. \n",
    "# Note that for 1001 there is one member_id, but for 1231 there are 5 member_ids\n",
    "# these refer to the individual ensemble members!\n",
    "\n",
    "dsets['ice.historical.cice.h1.cmip6.'+var_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the historical and future datasets\n",
    "\n",
    "historicals = []\n",
    "futures = []\n",
    "\n",
    "for key in sorted(dsets.keys()):\n",
    "    if 'historical' in key:\n",
    "        historicals.append(dsets[key])\n",
    "        print(key)\n",
    "    elif 'ssp370' in key:\n",
    "        futures.append(dsets[key])\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now put these into an array by member_id\n",
    "historical_ds = xr.concat(historicals, dim='member_id')\n",
    "future_ds = xr.concat(futures, dim='member_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the historical and future xarray datasets have the same coordinates and dimensions *except* time, \n",
    "# so we need to concatenate over time\n",
    "ds_ice = xr.concat([historical_ds,future_ds],dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to shift time by 1 day because of weird CESM conventions\n",
    "ds_ice = ds_ice.assign_coords(time=ds_ice.coords[\"time\"]-timedelta(days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Drop the lat/lons that we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the masking file\n",
    "ds_masks = xr.open_mfdataset('/glade/p/cgd/ppc/duvivier/masks/antarctic_ocean_masks_2.nc')\n",
    "\n",
    "# need to use the intersection of masks for a particular sector (e.g. Ross_mask) with the coastal mask (coast_mask)\n",
    "# create array for mask\n",
    "ds_mask = xr.where((ds_masks[mask_in]==1)&(ds_masks['coast_mask']==1),ds_masks['coast_mask'],0)\n",
    "\n",
    "# rename the coordinates for the mask\n",
    "ds_mask=ds_mask.rename({'nlat':'nj','nlon': 'ni'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the mask for correct value dropping\n",
    "mask_stacked = ds_mask.stack(horizontal=(\"nj\",\"ni\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ice_stacked = ds_ice[var_in].stack(horizontal=(\"nj\",\"ni\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ice_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now drop points that are masked\n",
    "ds_ice_masked = ds_ice_stacked.where(mask_stacked,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# actually load the data so it doesn't get too big later and makes DASK angry\n",
    "ds_ice_masked.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now also subset by some lat/lon values to better narrow down the region\n",
    "if sector_short == 'Wed':\n",
    "    ds_ice_masked_subset = ds_ice_masked.where(\n",
    "                             ((ds_ice_masked['TLAT']<lat_max) & (ds_ice_masked['TLAT']>lat_min)) &\\\n",
    "                             ((ds_ice_masked['TLON']<lon_min) | (ds_ice_masked['TLON']>lon_max)),\n",
    "                             drop=True)  \n",
    "else:\n",
    "    \n",
    "    ds_ice_masked_subset = ds_ice_masked.where(\n",
    "                             (ds_ice_masked['TLAT']<lat_max) & (ds_ice_masked['TLAT']>lat_min) & \\\n",
    "                             (ds_ice_masked['TLON']>lon_min) & (ds_ice_masked['TLON']<lon_max), \n",
    "                             drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ice_masked_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to check that we're plotting the correct area for the training data\n",
    "# using pcolor here\n",
    "\n",
    "# Choose just one timestep\n",
    "data = ds_ice_masked_subset.sel(member_id='r1i1281p1f1').isel(time=1000)\n",
    "\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "\n",
    "ax = plt.axes([0.,0.,1.,1.], projection=ccrs.SouthPolarStereo(central_longitude=0))\n",
    "\n",
    "ax.set_title(sector_title +' '+var_in, fontsize=12)\n",
    "\n",
    "# add cyclic point -- doesnt work due to nans\n",
    "#data_, lons_ = add_cyclic_point(data, coord=np.array(lon_new))\n",
    "\n",
    "# doing scatter instead for now\n",
    "cs1 = ax.scatter(     data.coords['TLON'].values,    \n",
    "                     data.coords['TLAT'].values, \n",
    "                     data, cmap='Blues',\n",
    "                vmin=0,vmax=5,\n",
    "                #vmin=-10,vmax=0,\n",
    "                     transform=ccrs.PlateCarree())\n",
    "\n",
    "ax.set_extent([lon_min,lon_max,lat_min,lat_max+10], ccrs.PlateCarree())\n",
    "\n",
    "############################################\n",
    "# Cartopy coastline and the land feature dont match perfectly for antarctica!\n",
    "# maybe just use one of them? i dont know which one is more accurate for your data \n",
    "\n",
    "ax.coastlines(resolution='110m', color='0.25', linewidth=0.5, zorder=10)  \n",
    "\n",
    "#ax.add_feature(cartopy.feature.LAND, zorder=10, edgecolor='k', facecolor='w')\n",
    "\n",
    "############################################\n",
    "\n",
    "ax.gridlines(linestyle='--', linewidth=0.5, zorder=11)\n",
    "\n",
    "#plt.colorbar(cs1)\n",
    "\n",
    "#plt.savefig(sector_short+'_'+var_in+'_1.png', bbox_inches='tight', dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Subset the times we want to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ice_masked_subset.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just years greater than 1980 and less than 2080 \n",
    "yy_st = \"1980\"\n",
    "yy_ed = \"2080\"\n",
    "ds_ice_masked_subset = ds_ice_masked_subset.sel(time=slice(yy_st, yy_ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ice_masked_subset.time.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just times corresponding to winter (SH: jul, aug, sept)\n",
    "ds_ice_winter = ds_ice_masked_subset.isel(time=ds_ice_masked_subset.time.dt.month.isin([7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ice_winter.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# actually load the data so it doesn't get too big later and makes DASK angry\n",
    "ds_ice_winter.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Get data into format needed by miniSOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ice_winter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the times and member_id\n",
    "training_subset = ds_ice_winter.stack(new=(\"member_id\",\"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_subset = training_subset.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign to numpy array object\n",
    "subsetarray = training_subset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triple check the data dims/shape\n",
    "print(subsetarray.shape)\n",
    "# confirm there are no NaN values in array for training (should print False if no values)\n",
    "print(np.isnan(subsetarray).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Save data as a netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = 'training_data_region_'+sector_short+'_'+var_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_save = xr.Dataset({'train_data': (['training_times','points'], subsetarray)}, \n",
    "                        coords={'time':(['training_times'],training_subset.time.values),\n",
    "                                'member_id':(['training_times'],training_subset.member_id.values),\n",
    "                                'TLON':(['points'],training_subset.TLON.values),\n",
    "                                'TLAT':(['points'],training_subset.TLAT.values),\n",
    "                                'nj':(['points'],training_subset.nj.values),\n",
    "                                'ni':(['points'],training_subset.ni.values)},\n",
    "                        attrs={'Author': 'Alice DuVivier'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_save.to_netcdf(fout+'.nc')  # how to save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-antarctica_som_env]",
   "language": "python",
   "name": "conda-env-miniconda3-antarctica_som_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
