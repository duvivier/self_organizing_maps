{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Organizing Maps (SOMs) Notebook\n",
    "## Training step - Step 2\n",
    "\n",
    "**Notebook by Maria J. Molina (NCAR) and Alice DuVivier (NCAR).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook reads in data subset for a particular region and variable, done in step 1. Then it loops through a series of SOM hyperparameters to train a number of SOMs and determine the best size and such to answer the science questions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed imports\n",
    "\n",
    "from minisom import MiniSom, asymptotic_decay\n",
    "import xarray as xr\n",
    "import cftime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "from datetime import timedelta\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set user-specified information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set: variable to test, the location of the already extracted training data\n",
    "var_in = 'aice_d'\n",
    "sector_short = 'Ross'\n",
    "data_path = '/glade/p/cgd/ppc/duvivier/cesm2_antarctic_polynya/SOM_analysis/training/'+sector_short+'_v5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and get correct training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file saved in earlier notebook (soms_antarctica-gettingdata.ipynb)\n",
    "subset = xr.open_dataset(data_path+'training_data_region_'+sector_short+'_'+var_in+'.nc')\n",
    "\n",
    "# assign to numpy array object\n",
    "subsetarray = subset['train_data'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data dims/shape - should match the dims from the getting data notebook\n",
    "print(subsetarray.shape)\n",
    "# confirm there are no NaN values in array for training (should print False if no values)\n",
    "print(np.isnan(subsetarray).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: SOM training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set SOM Hyperparameters we'll test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set possible grid sizes. These are paired values.\n",
    "som_grid_rows    = [3, 4, 5]    # (y-axis)\n",
    "som_grid_cols    = [3, 4, 5]    # (x-axis)\n",
    "\n",
    "# for each SOM grid, we will need to all possible combos with parameters below\n",
    "# spread of neighborhood function - sigma - is set below and depends on the som shape\n",
    "###sigma            = [1.0, 0.5, 0.25, 0.1]\n",
    "# initial learning rate (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)\n",
    "learning_rate    = [0.005, 0.01, 0.04, 0.05, 0.5]\n",
    "# how many iterations to go through\n",
    "num_iteration    = [10000, 50000, 100000, 250000, 500000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the normalizing data function\n",
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    Function for normalizing data prior to training using z-score\n",
    "    \"\"\"\n",
    "    return (data - np.nanmean(data)) / np.nanstd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set other attributes required for som training\n",
    "input_length = subsetarray.shape[1]      # Total number of points to train on per timestep\n",
    "decay_function = asymptotic_decay        # Function that reduces learning_rate and sigma at each iteration\n",
    "neighborhood_function = 'gaussian'       # Function that weights the neighborhood of a position in the map\n",
    "topology = 'rectangular'                 # Topology of the map; Possible values: 'rectangular', 'hexagonal'\n",
    "activation_distance = 'euclidean'        # Distance used to activate the map; Possible values: 'euclidean', 'cosine', 'manhattan', 'chebyshev'\n",
    "random_seed = 1                          # Random seed to use for reproducibility. Using 1.\n",
    "random_order = True\n",
    "verbose = False #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want to loop through the different SOM grid sizes and train each separately and write out CSV file\n",
    "\n",
    "for num_grid, (som_row,som_col) in enumerate(zip(som_grid_rows,som_grid_cols)):\n",
    "    #print(num_grid)\n",
    "    print('Testing grid: '+str(som_row)+'x'+str(som_col))\n",
    "\n",
    "    # set csv filename based on \n",
    "    fout = 'test_soms_qerror_'+sector_short+'_'+var_in+'_'+str(som_row)+'x'+str(som_col)+'.csv'\n",
    "    # array to put quantization errors for this grid\n",
    "    quant_errors = []\n",
    "    # set sigma values based on this som shape\n",
    "    sigma = list(np.hstack([0.5,list(range(1,som_col,1))]))\n",
    "\n",
    "    # creating list of hyperparameters for each SOM grid\n",
    "    list_of_sigs = []\n",
    "    list_of_lrts = []\n",
    "    list_of_itrs = []\n",
    "    for sig, lr, n_iter in product(sigma, learning_rate, num_iteration):\n",
    "        list_of_sigs.append(sig)\n",
    "        list_of_lrts.append(lr)\n",
    "        list_of_itrs.append(n_iter)\n",
    "    \n",
    "    # create empty csv\n",
    "    our_csv = pd.DataFrame(np.zeros((len(list_of_sigs), 6), dtype=int), columns=[\"n_row\", \"n_col\", \"sigma\", \"lr\", \"n_iter\", \"q_error\"])\n",
    "\n",
    "    \n",
    "    # now loop through the training parameters for this specific grid\n",
    "    for num_exp, (sig, lr, n_iter) in enumerate(zip(list_of_sigs,list_of_lrts,list_of_itrs)):\n",
    "        # print out which experiment for this grid we are on\n",
    "        print('Testing '+str(num_exp+1)+' of '+str(len(list_of_sigs)))\n",
    "        # normalize the training data - maybe take out??\n",
    "        data = normalize_data(subsetarray)\n",
    "        # initialize the SOM    \n",
    "        som = MiniSom(som_row,som_col,input_length,sig,lr,decay_function,\n",
    "                      neighborhood_function,topology,activation_distance,random_seed) \n",
    "    \n",
    "        som.pca_weights_init(data)  # Initializes the weights to span the first two principal components\n",
    "                                    # could also try random init: som.random_weights_init(data)\n",
    "        # train the SOM!\n",
    "        som.train(data,n_iter,random_order,verbose)\n",
    "        # Add to the csv file\n",
    "        our_csv.iloc[num_exp] += [som_row, som_col, sig, lr, n_iter, som.quantization_error(data)]    \n",
    "    \n",
    "    # Write the csv file for this grid after testing all the combinations\n",
    "    print('Writing out CSV file with all qerror for som grid '+str(som_row)+'x'+str(som_col))\n",
    "    our_csv.to_csv(fout)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-antarctica_som_env]",
   "language": "python",
   "name": "conda-env-miniconda3-antarctica_som_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
