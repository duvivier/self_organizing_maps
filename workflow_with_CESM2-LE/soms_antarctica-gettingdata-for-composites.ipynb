{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Organizing Maps (SOMs) Notebook\n",
    "## Data extraction for composites\n",
    "\n",
    "**Notebook by Maria J. Molina (NCAR) and Alice DuVivier (NCAR).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook reads in data from the CESM2-LE for a user-specified variable. It subsets the data to be just around Antarctica to create composites from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed imports\n",
    "\n",
    "from minisom import MiniSom, asymptotic_decay\n",
    "import xarray as xr\n",
    "import cftime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.util import add_cyclic_point\n",
    "from datetime import timedelta\n",
    "from itertools import product\n",
    "\n",
    "import intake\n",
    "from distributed import Client\n",
    "from ncar_jobqueue import NCARCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.__version__\n",
    "# Did a conda update dask on cheyenne to get to 2021.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start up dask\n",
    "\n",
    "cluster = NCARCluster(memory='100 GB', walltime='1:00:00', cores=4, processes=2, resource_spec='select=1:ncpus=2:mem=100GB')\n",
    "cluster.scale(40) # number of workers requested\n",
    "#cluster.adapt(1,80) # min and max\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster = NCARCluster(memory=\"100GB\", walltime='8:00:00', cores=4, processes=2, resource_spec='select=1:ncpus=2:mem=100GB')\n",
    "Each worker has 100GB, resource_spec is assigning this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "\n",
    "#Set dask.config.set({\"array.slicing.split_large_chunks\": False}) to allow the large chunk and silence the warning.\n",
    "\n",
    "#### HERE - MAX SAID TRY THIS TOO (BELOW)\n",
    "# with dask.congfig.set... (DO RIGHT BEFORE ACTUALLY READING DATA WITH DATASET DICT)\n",
    "#Set dask.config.set({\"array.slicing.split_large_chunks\": True}) to avoid creating the large chunk in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and get correct training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some info for the CESM2-LE data\n",
    "# set: variable to test, the location of the data, which ensemble member\n",
    "var_in = 'hi_d'\n",
    " # do not want smbb data\n",
    "forcing = 'cmip6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_file = '/glade/collections/cmip/catalog/intake-esm-datastore/catalogs/glade-cesm2-le.json'\n",
    "\n",
    "cat = intake.open_esm_datastore(catalog_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = cat.search(variable=var_in, forcing_variant=forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset\n",
    "subset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make arrays of half (25) of the CESM2-LE members \n",
    "# select every other from the large ensemble of both macro and micro starts\n",
    "# note that the naming of the files (YYYY.#### e.g. 1001.001) doesn't match the member_id directly, \n",
    "# but the ensemble number (### e.g. 001) does match the member_id field r? directly. So use this to search\n",
    "\n",
    "# set list of members from the dataset\n",
    "member_ids = subset.df.member_id.unique()\n",
    "\n",
    "# set list of members to KEEP\n",
    "keep_list = ['r1i', 'r3i', 'r5i','r7i', 'r9i']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_keep = [] # make a list to fill\n",
    "\n",
    "for member in keep_list:\n",
    "    for member_id in member_ids:\n",
    "        if member in member_id:\n",
    "            member_keep.append(member_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that we're keeping the right ones\n",
    "member_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reduce subset based on just the members to keep\n",
    "subset = subset.search(member_id=member_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#actually load the data we selected into a dataset\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "    dsets = subset.to_dataset_dict(cdf_kwargs={'chunks': {'time':240}, 'decode_times': True})\n",
    "\n",
    "#dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print names of the dataset keys, which refer to each of the ensembles loaded\n",
    "dsets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just one dataset key to see what it looks like. \n",
    "# Note that for 1001 there is one member_id, but for 1231 there are 5 member_ids\n",
    "# these refer to the individual ensemble members!\n",
    "\n",
    "dsets['ice.historical.cice.h1.cmip6.'+var_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the historical and future datasets\n",
    "\n",
    "historicals = []\n",
    "futures = []\n",
    "\n",
    "for key in sorted(dsets.keys()):\n",
    "    if 'historical' in key:\n",
    "        historicals.append(dsets[key])\n",
    "        print(key)\n",
    "    elif 'ssp370' in key:\n",
    "        futures.append(dsets[key])\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now put these into an array by member_id\n",
    "historical_ds = xr.concat(historicals, dim='member_id')\n",
    "future_ds = xr.concat(futures, dim='member_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the historical and future xarray datasets have the same coordinates and dimensions *except* time, \n",
    "# so we need to concatenate over time\n",
    "ds = xr.concat([historical_ds,future_ds],dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to shift time by 1 day because of weird CESM conventions\n",
    "ds = ds.assign_coords(time=ds.coords[\"time\"]-timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Drop the lats that we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some limits for Antarctica in general\n",
    "lat_max = -60\n",
    "lat_min = -80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now drop points that are masked\n",
    "ds_subset = ds.where(((ds['TLAT']<lat_max) & (ds['TLAT']>lat_min)), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset = ds_subset[var_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have all of Antarctica here\n",
    "ds_subset.sel(member_id='r1i1281p1f1').isel(time=1000).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "## actually load the data so it doesn't get too big later and makes DASK angry\n",
    "#ds_subset.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Subset the times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just years greater than 1980 and less than 2080 \n",
    "yy_st = \"1980\"\n",
    "yy_ed = \"2080\"\n",
    "ds_subset = ds_subset.sel(time=slice(yy_st, yy_ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset.time.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just times corresponding to winter (SH: all times between april and sept)\n",
    "ds_subset_winter = ds_subset.isel(time=ds_subset.time.dt.month.isin([7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset_winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# actually load the data so it doesn't get too big later and makes DASK angry\n",
    "#ds_subset_winter.persist()\n",
    "ds_subset_winter.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Save data for making composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset_winter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the times and member_id\n",
    "subset_for_composites = ds_subset_winter.stack(new=(\"member_id\",\"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign to numpy array object\n",
    "subsetarray = subset_for_composites.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_for_composites.TLAT.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Save data as a netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = 'antarctic_data_for_som_composites_'+var_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some info for output\n",
    "longname = subset_for_composites.long_name\n",
    "print(longname)\n",
    "units = subset_for_composites.units\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_save = xr.Dataset({'data': (['nj','ni','training_times'], subsetarray)}, \n",
    "                        coords={'time':(['training_times'],subset_for_composites.time.values),\n",
    "                                'member_id':(['training_times'],subset_for_composites.member_id.values),\n",
    "                                'TLON':(['nj','ni'],subset_for_composites.TLON.values),\n",
    "                                'TLAT':(['nj','ni'],subset_for_composites.TLAT.values),\n",
    "                                'nj':(['nj'],subset_for_composites.nj.values),\n",
    "                                'ni':(['ni'],subset_for_composites.ni.values)},\n",
    "                        attrs={'Author': 'Alice DuVivier', 'units':units, 'longname':longname})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_save.to_netcdf(fout+'.nc')  # how to save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-antarctica_som_env]",
   "language": "python",
   "name": "conda-env-miniconda3-antarctica_som_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
