{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Organizing Maps (SOMs) Notebook\n",
    "\n",
    "**Notebook by Maria J. Molina (NCAR) and Alice DuVivier (NCAR).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook reads in data from a single CESM2-LE member for a user-specified variable. It subsets the data by a user-specified coastal region around Antarctica. Then it loops through a series of SOM hyperparameters to train a number of SOMs. There is also code to evaluate the SOM robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before starting, look at the directory on setting up your environment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed imports\n",
    "\n",
    "from minisom import MiniSom, asymptotic_decay\n",
    "import xarray as xr\n",
    "import cftime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from datetime import timedelta\n",
    "from itertools import product\n",
    "import pickle\n",
    "import sammon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set user-specified information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER SPECIFIED DATA CHOICES\n",
    "\n",
    "# choose what variable to train on with daily data\n",
    "var_in = 'aice_d'\n",
    "# set where the data are located\n",
    "data_root= '/glade/campaign/cgd/cesm/CESM2-LE/timeseries/ice/proc/tseries/day_1/'\n",
    "# select a single CESM2-LE member\n",
    "ens_name='b.e21.BHISTcmip6.f09_g17.LE2-1001.001'\n",
    "\n",
    "# set path for masks\n",
    "data_mask = './'\n",
    "mask_name = 'antarctic_ocean_masks_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which region to train SOM on\n",
    "\n",
    "# these values are needed for choosing training area and then plotting\n",
    "sector_short = 'Ross'\n",
    "mask_in = 'Ross_mask'\n",
    "lat_max = -72\n",
    "lat_min = -85\n",
    "lon_max = 200\n",
    "lon_min = 160\n",
    "lon_avg = 190\n",
    "\n",
    "# select years and months to subset data\n",
    "# select just winter: JAS (7,8,9) \n",
    "mm_st = 7\n",
    "mm_ed = 9\n",
    "# select just 1900-1950\n",
    "yy_st = 1900\n",
    "yy_ed = 1950"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load and get correct training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open multiple datasets and automatically combine if there are no conflicts\n",
    "ds = xr.open_mfdataset(data_root+var_in+'/'+ens_name+'.cice.h1.'+var_in+'.*.nc', combine='by_coords')\n",
    "\n",
    "# Shifting time by 1 day because CESM saves data at the end of a time period\n",
    "ds = ds.assign_coords(time=ds.coords[\"time\"]-timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select just the variable we are training on to keep\n",
    "ds_ice = ds[var_in]\n",
    "\n",
    "# subset by months\n",
    "ds_ice = ds_ice[(ds_ice.coords['time.month']>=mm_st)&(ds_ice.coords['time.month']<=mm_ed)]\n",
    "\n",
    "# subset by years\n",
    "ds_ice = ds_ice[(ds_ice.coords['time.year']>=yy_st)&(ds_ice.coords['time.year']<=yy_ed)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the time bounds look correct\n",
    "ds_ice.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the masking file\n",
    "ds_masks = xr.open_mfdataset(data_mask+mask_name+'.nc')\n",
    "\n",
    "# create array for mask\n",
    "# need to use the intersection of masks for a particular sector (e.g. Ross_mask) with the coastal mask (coast_mask)\n",
    "ds_mask = xr.where((ds_masks[mask_in]==1)&(ds_masks['coast_mask']==1),ds_masks['coast_mask'],0)\n",
    "\n",
    "# mask the ice data with the regional, coastal mask\n",
    "ds_ice_masked = xr.where(ds_mask.values==1,ds_ice,-999.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make smaller array where we ignore all global data - this makes it easier to plot\n",
    "ds_ice_masked_subset = ds_ice_masked.where(\n",
    "                             (ds['TLAT']<lat_max) & (ds['TLAT']>lat_min) & \\\n",
    "                             (ds['TLON']>lon_min) & (ds['TLON']<lon_max), \n",
    "                             drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we've selected the correct area for the training data - use pcolor\n",
    "\n",
    "# set labeling info:\n",
    "fout = sector_short+'_'+var_in\n",
    "\n",
    "# Choose just one timestep\n",
    "data = ds_ice_masked_subset.isel(time=0)\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = plt.axes([0.,0.,1.,1.], projection=ccrs.Stereographic(central_longitude=lon_avg))\n",
    "# make plot\n",
    "cs1 = ax.pcolor(     data.coords['TLON'].values,    \n",
    "                     data.coords['TLAT'].values, \n",
    "                     data, \n",
    "                     cmap='Blues',vmin=0,vmax=1,\n",
    "                     transform=ccrs.PlateCarree())\n",
    "# select some regional boundaries to plot\n",
    "ax.set_title(fout, fontsize=12)\n",
    "ax.set_extent([lon_min,lon_max,lat_min,lat_max], ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='110m', color='0.25', linewidth=0.5, zorder=10)  \n",
    "ax.gridlines(linestyle='--', linewidth=0.5, zorder=11)\n",
    "# add a colorbar\n",
    "plt.colorbar(cs1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# uncomment below to save figure\n",
    "#plt.savefig(fout+'_1.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we've selected the correct area for the training data - use scatter plot to see individual grid points\n",
    "\n",
    "# set labeling info:\n",
    "fout = sector_short+'_'+var_in\n",
    "\n",
    "# Choose just one timestep\n",
    "data = ds_ice_masked_subset.isel(time=0)\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = plt.axes([0.,0.,1.,1.], projection=ccrs.Stereographic(central_longitude=lon_avg))\n",
    "# make plot\n",
    "cs1 = ax.scatter(    data.coords['TLON'].values,    \n",
    "                     data.coords['TLAT'].values, \n",
    "                     data, \n",
    "                     cmap='Blues',vmin=0,vmax=1,\n",
    "                     transform=ccrs.PlateCarree())\n",
    "# select some regional boundaries to plot\n",
    "ax.set_title(fout, fontsize=12)\n",
    "ax.set_extent([lon_min,lon_max,lat_min,lat_max], ccrs.PlateCarree())\n",
    "ax.coastlines(resolution='110m', color='0.25', linewidth=0.5, zorder=10)  \n",
    "ax.gridlines(linestyle='--', linewidth=0.5, zorder=11)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# uncomment below to save figure\n",
    "#plt.savefig(fout+'_2.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually load data\n",
    "ds_ice_masked_subset = ds_ice_masked_subset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# THIS STEP IS A SLOW ONE\n",
    "# Flatten into this new shape * prior * to dropping values. otherwise, xarray fills \n",
    "# values with NaNs (or other value) to return a 2d shape, which we don't want.\n",
    "ds_ice_masked_1d = ds_ice_masked.stack(new=(\"nj\",\"ni\"))\n",
    "\n",
    "# assign object a name (e.g., subset) and drop the data we don't need to minimize size of the array\n",
    "subset = ds_ice_masked_1d.where(ds_ice_masked_1d!=-999.999, drop=True)\n",
    "\n",
    "# assign to numpy array object\n",
    "subsetarray = subset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triple check the data dimensions\n",
    "print(subsetarray.shape)\n",
    "# confirm there are no NaN values in array for training (should print False if no values)\n",
    "print(np.isnan(subsetarray).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Train the SOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set SOM Hyperparameters we'll test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set possible grid sizes - these should be equal so the SOM is square and are paired below\n",
    "# user should try different combinations - e.g. 3x3, 5x5, 9x9\n",
    "som_grid_rows    = [3]    # (y-axis)\n",
    "som_grid_columns = [3]    # (x-axis)\n",
    "\n",
    "# spread of neighborhood function - largest value should be one smaller than som dimension, decrease by one after that\n",
    "sigma            = [2.0, 1.0, 0.5]\n",
    "# initial learning rate (at the iteration t we have learning_rate(t) = learning_rate / (1 + t/T) where T is #num_iteration/2)\n",
    "learning_rate    = [0.005, 0.05, 0.5]\n",
    "# how many iterations to go through\n",
    "num_iteration    = [100000, 500000, 1000000]\n",
    "\n",
    "# for above values, miniSOM will test test a total of 3*3*3 = 27 possible SOMs for this size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of hyperparameters to iterate through\n",
    "list_of_rows = []\n",
    "list_of_cols = []\n",
    "list_of_sigs = []\n",
    "list_of_lrts = []\n",
    "list_of_itrs = []\n",
    "\n",
    "for som_row, som_col in zip(som_grid_rows, som_grid_columns):\n",
    "    for sig, lr, n_iter in product(sigma, learning_rate, num_iteration):\n",
    "        list_of_rows.append(som_row)\n",
    "        list_of_cols.append(som_col)\n",
    "        list_of_sigs.append(sig)\n",
    "        list_of_lrts.append(lr)\n",
    "        list_of_itrs.append(n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to normalize data\n",
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    Function for normalizing data prior to training using z-score\n",
    "    \"\"\"\n",
    "    return (data - np.nanmean(data)) / np.nanstd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually train som\n",
    "# this step is slow because it's training all those combinations and saving the quantization errors\n",
    "\n",
    "quant_errors = []\n",
    "\n",
    "random_order = True\n",
    "verbose = True\n",
    "\n",
    "# empty csv\n",
    "our_csv = pd.DataFrame(np.zeros((len(list_of_rows), 6), dtype=int), columns=[\"n_row\", \"n_col\", \"sigma\", \"lr\", \"n_iter\", \"q_error\"])\n",
    "\n",
    "for num_exp, (som_row, som_col, sig, lr, n_iter) in enumerate(zip(list_of_rows,list_of_cols,list_of_sigs,list_of_lrts,list_of_itrs)):\n",
    "    # print out which SOM we are on\n",
    "    print(num_exp)\n",
    "    \n",
    "    # set other attributes required for som training\n",
    "    input_length = subsetarray.shape[1]      # Total number of points to train on per timestep\n",
    "    decay_function = asymptotic_decay        # Function that reduces learning_rate and sigma at each iteration\n",
    "    neighborhood_function = 'gaussian'       # Function that weights the neighborhood of a position in the map\n",
    "    topology = 'rectangular'                 # Topology of the map; Possible values: 'rectangular', 'hexagonal'\n",
    "    activation_distance = 'euclidean'        # Distance used to activate the map; Possible values: 'euclidean', 'cosine', 'manhattan', 'chebyshev'\n",
    "    random_seed = 1                          # Random seed to use for reproducibility. Using 1.\n",
    "    random_order = True\n",
    "    verbose = True\n",
    "    \n",
    "    # initialize the SOM    \n",
    "    som = MiniSom(som_row,som_col,input_length,sig,lr,decay_function,\n",
    "                  neighborhood_function,topology,activation_distance,random_seed) \n",
    "    \n",
    "    data = normalize_data(subsetarray)  # prob take out\n",
    "    \n",
    "    som.pca_weights_init(data)  # Initializes the weights to span the first two principal components\n",
    "                                # could also try random init: som.random_weights_init(data)\n",
    "    \n",
    "    # train the SOM!\n",
    "    som.train(data,n_iter,random_order,verbose)\n",
    "    \n",
    "    print('yay! som training complete')\n",
    "    \n",
    "    our_csv.iloc[num_exp] += [som_row, som_col, sig, lr, n_iter, som.quantization_error(data)]\n",
    "        \n",
    "    print('on to the next one...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the CSV in case we need it later\n",
    "fout = 'som_qerror_'+sector_short+'_'+var_in+'_'+str(som_grid_rows[0])+'x'+str(som_grid_columns[0])+'.csv'\n",
    "our_csv.to_csv(fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Evaluate the SOMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the qerrors to find lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv file with all the possible soms\n",
    "df = pd.read_csv(fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values by q_error\n",
    "sorted_df = df.sort_values(['q_error'])\n",
    "\n",
    "# grab all the qerrors and make array against which to plot\n",
    "qerr_all = sorted_df.q_error\n",
    "xarr_all = np.arange(1,len(qerr_all)+1,1)\n",
    "\n",
    "# find and save the lowest qerror for top # (1)\n",
    "top_n = sorted_df.head(1)\n",
    "bottom_n = sorted_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the training values for this winning combination\n",
    "qerr = top_n['q_error'].item()\n",
    "sig = top_n['sigma'].item()\n",
    "lr = top_n['lr'].item()\n",
    "n_iter = int(top_n['n_iter'].item())\n",
    "\n",
    "# set file name based on these combos\n",
    "fin = 'som_'+sector_short+'_'+var_in+'_'+str(som_row)+'x'+str(som_col)+'_sig'+str(sig)+'_lr'+str(lr)+'_iter'+str(n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting qerror for som: '+str(som_row)+'x'+str(som_col))\n",
    "\n",
    "# Actually plot figure now\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "# plot all SOM qerror\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.scatter(xarr_all,qerr_all,marker='x',c='black')\n",
    "plt.title('all qerror',fontsize=15)\n",
    "plt.xlabel('ranking',fontsize=15)\n",
    "plt.ylabel('qerror',fontsize=15)\n",
    "\n",
    "# Finalize figure and save\n",
    "fig.suptitle('qerror for SOM',fontsize=20, y=0.95)  \n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# uncomment below to save figure\n",
    "#plt.savefig('qerrors_all.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train lowest qerror combination so we can save the SOM and use it to plot more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('re-training lowest qerror only')\n",
    "    \n",
    "# print the qerror read in. This should match the final qerror after re-training.\n",
    "print('original qerr = '+str(qerr))\n",
    "\n",
    "# initialization of SOM\n",
    "som = MiniSom(\n",
    "            som_row,\n",
    "            som_col,\n",
    "            input_length,\n",
    "            sig,\n",
    "            lr,\n",
    "            decay_function,\n",
    "            neighborhood_function,\n",
    "            topology,\n",
    "            activation_distance,\n",
    "            random_seed) \n",
    "# before training, initialize the data\n",
    "som.pca_weights_init(data) \n",
    "# actually train SOM - the quantization error here should match qerr printed above\n",
    "som.train(\n",
    "        data,\n",
    "        n_iter,\n",
    "        random_order,\n",
    "        verbose)\n",
    "    \n",
    "# save the som as a pickle to analyze later\n",
    "with open(fin+'.p', 'wb') as outfile:\n",
    "    pickle.dump(som, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sammon map\n",
    "\n",
    "This shows how \"flat\" the map is in 2D space. You do NOT want a twisted map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle\n",
    "with open(fin+'.p', 'rb') as infile:\n",
    "    som = pickle.load(infile)\n",
    "\n",
    "# Calculate sammon coordinates (y) for map and \"map stress\" (E)\n",
    "[y,E] = sammon.sammon(som.get_weights().reshape(som_col*som_row, input_length),2,display=1)\n",
    "# Plot Sammon map nodes\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.scatter(y[:,0], y[:,1], s=20, c='black', marker='o')\n",
    "# Add lines between nodes\n",
    "tmp = np.reshape(y,(som_col,som_row,2))\n",
    "len_x, len_y, len_z = tmp.shape\n",
    "# add vertical lines\n",
    "for i in range(len_x-1):\n",
    "    for j in range(len_y):\n",
    "        plt.plot(tmp[i:i+2,j,0],tmp[i:i+2,j,1],c='black')\n",
    "# add horizontal lines\n",
    "for i in range(len_x):\n",
    "    for j in range(len_y-1):\n",
    "        plt.plot(tmp[i,j:j+2,0],tmp[i,j:j+2,1],c='black')    \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(r\"sammon map\" \"\\n\" r\"map stress = \"+str(E), fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# uncomment below to save figure\n",
    "#plt.savefig(fin+'_sammon.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the node frequencies\n",
    "\n",
    "This shows you how frequently patterns from the training data mapped to each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pickle\n",
    "with open(fin+'.p', 'rb') as infile:\n",
    "    som = pickle.load(infile)\n",
    "\n",
    "# set frequencies\n",
    "frequencies = 100.*((som.activation_response(data))/sum(sum(som.activation_response(data))))\n",
    "#verify the total frequency is 100%\n",
    "total = sum(sum(frequencies))\n",
    "\n",
    "# Plot frequencies across SOM lattice\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = plt.subplot(111)\n",
    "im = ax.imshow(frequencies, cmap='Blues')    \n",
    "# Loop over data dimensions and create text annotations in each cell\n",
    "len_x, len_y = frequencies.shape\n",
    "for i in range(len_x):\n",
    "    for j in range(len_y):\n",
    "        text = ax.text(j, i, str(round(frequencies[i, j],1))+'%', fontsize=15, ha=\"center\", va=\"center\", color=\"k\")\n",
    "\n",
    "# Make cosmetic changes\n",
    "cbar = plt.colorbar(im)\n",
    "plt.title(\"data frequency (2d histogram) across SOM lattice\" \"\\n\" r\"total frequency = \"+str(total)+\"%\", fontsize=12)\n",
    "plt.xticks(np.arange(0,som_row, 1))\n",
    "plt.yticks(np.arange(0,som_col, 1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# uncomment below to save figure\n",
    "#plt.savefig(fin+'_freq.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a composite map\n",
    "\n",
    "This shows you what the average of each node looks like so you can see the patterns the SOM identified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing indices from SOM\n",
    "# open pickle\n",
    "with open(fin+'.p', 'rb') as infile:\n",
    "    som = pickle.load(infile)\n",
    "\n",
    "# create an empty dictionary using the rows and columns of SOM\n",
    "keys = [i for i in product(range(som_row),range(som_col))]\n",
    "winmap = {key: [] for key in keys}\n",
    "\n",
    "# grab the indices for the data within the SOM lattice\n",
    "for i, x in enumerate(data):\n",
    "    winmap[som.winner(x)].append(i)\n",
    "\n",
    "som_keys = list(winmap.keys())\n",
    "print(f\"Number of composite maps: {len(som_keys)}\")\n",
    "print(f\"The rows and columns of the SOM lattice to use to grab SOM indexes:\\n{[i for i in list(winmap.keys())]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab first node - needed later for TLAT/TLON since that was misbehaving. Ugh\n",
    "node1 = ds_ice_masked_subset.stack(new=(\"nj\",\"ni\"))[np.array(winmap[som_keys[0]])].unstack().mean(dim=\"time\",skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all SOM composite maps   (this cell takes a few minutes to run, especially if SOM lattice is large)\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "fig, axs = plt.subplots(som_row, som_col, subplot_kw={'projection':ccrs.Stereographic(central_longitude=lon_avg)}, figsize=(14,12))\n",
    "\n",
    "# set the colors\n",
    "cmap_choice = plt.cm.get_cmap('coolwarm')\n",
    "cmap_choice.set_bad(color='white')\n",
    "\n",
    "# ------------------------\n",
    "\n",
    "for map_num in range(len(som_keys)):\n",
    "    print(\"Making map: \"+str(map_num))\n",
    "    \n",
    "    # the data for this node\n",
    "    temp_data = ds_ice_masked_subset.stack(new=(\"nj\", \"ni\"))[np.array(winmap[som_keys[map_num]])].unstack().mean(dim=\"time\",skipna=True).values\n",
    "    \n",
    "    # plot\n",
    "    cs = axs[som_keys[map_num][0],som_keys[map_num][1]].pcolor(\n",
    "                                                              node1.coords['TLON'].values, \n",
    "                                                              node1.coords['TLAT'].values, \n",
    "                                                              temp_data, \n",
    "                                                              #vmin=0, vmax=2, cmap=cmap_choice,   # set vmin and vmax so that all plots are on same scale (for colorbar)\n",
    "                                                              vmin=0, vmax=1, cmap=cmap_choice,   # set vmin and vmax so that all plots are on same scale (for colorbar)\n",
    "                                                              transform=ccrs.PlateCarree())\n",
    "\n",
    "    axs[som_keys[map_num][0],som_keys[map_num][1]].set_extent([lon_min,lon_max,lat_min,lat_max])\n",
    "    axs[som_keys[map_num][0],som_keys[map_num][1]].coastlines(resolution='110m', color='0.25', linewidth=0.5, zorder=10)\n",
    "    axs[som_keys[map_num][0],som_keys[map_num][1]].add_feature(cartopy.feature.LAND, zorder=10, edgecolor='k', facecolor='w')    \n",
    "    axs[som_keys[map_num][0],som_keys[map_num][1]].gridlines(linestyle='--', linewidth=0.5, zorder=11)\n",
    "    \n",
    "    # plot titles\n",
    "    axs[som_keys[map_num][0],som_keys[map_num][1]].set_title('Node Frequency (%):{:.2f}'.format(frequencies.flatten()[map_num]), fontsize=12)\n",
    "\n",
    "# ------------------------\n",
    "    \n",
    "# figure title\n",
    "plt.suptitle(sector_short+' SOM '+var_in+' node composites', fontsize=12, x=0.515, y=0.925)\n",
    "\n",
    "# colorbar stuff\n",
    "cbar_ax = fig.add_axes([0.25,0.1,0.5,0.01])  # set axis for colorbar e.g., ([x, y, dx, dy])\n",
    "ticks_1 = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]     # cb ticks (use same scale as vmin and vmax)\n",
    "cbar = fig.colorbar(cs, # (from plot loop above, notice we equaled plot to cs -- this passes those attributes to here)\n",
    "                    cax=cbar_ax, ticks=ticks_1[:],     # plot it\n",
    "                    orientation='horizontal', extend='both')\n",
    "cbar.ax.set_xticklabels([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] )   # tick labels\n",
    "cbar.ax.tick_params(labelsize=12)     # tick size\n",
    "cbar.set_label('ice concentration (frac)', fontsize=12)    # cb label\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# uncomment below to save figure\n",
    "#plt.savefig(fin+'_composites.png', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-antarctica_som_env]",
   "language": "python",
   "name": "conda-env-miniconda3-antarctica_som_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
